{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import math\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import csv\n",
    "import fnmatch\n",
    "import os\n",
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def one_row_n_grams(wordlist, n, trim=True, vector_cell = True):\n",
    "        # print('----------------------------------')\n",
    "        # print('One row wordlist: ', wordlist, len(wordlist))\n",
    "        if (vector_cell):\n",
    "            # print('vector_cell: ', vector_cell, 'Wordlist:', wordlist)\n",
    "            wordlist = ''.join(wordlist)\n",
    "            # print('Wordlist: ', wordlist)\n",
    "        try:\n",
    "            if (wordlist[0] == 'b'):\n",
    "                wordlist = wordlist[1:]\n",
    "        except:\n",
    "            # print('except')\n",
    "            return ()\n",
    "        if(trim):\n",
    "            wordlist = wordlist[1:-1]\n",
    "        # print('wordlist:', wordlist)\n",
    "        # print('len:',len(wordlist.split()))\n",
    "        if(len(wordlist.split()) < n):\n",
    "            n = len(wordlist.split())\n",
    "        t_list = []\n",
    "        for i in range(len(wordlist.split())-(n-1)):\n",
    "            temp = wordlist.lower().replace('\"', '').replace(\"'\", \"\").replace('!', '') \\\n",
    "                  .replace(\"?\", \"\").replace(\",\", \"\").replace(\".\", \"\").replace(\"#\", \"\") \\\n",
    "                       .replace(\"@\", \"\").replace(\":\", \"\").replace(\";\", \"\").replace(\"/\", \"\")\\\n",
    "                       .replace(\"-\", \"\").replace(\"\\\\n\", \"\").replace(\"\\\\\",\"\").replace(\"~\",\"\").split()[i:i+n]\n",
    "            # temp = temp.replace(\"\\\\\", \"\")\n",
    "            # print('temp:', temp)\n",
    "            if(len(temp)> 0):\n",
    "                t_list.append(temp)\n",
    "        # print('t_list:',t_list)\n",
    "        return (t_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    " def match_ngrams(vector, level, n, trim, vector_cell):\n",
    "        # print('n:',n)\n",
    "        result = []\n",
    "        # for item in vector:\n",
    "        for row in list(vector):\n",
    "            # print('----------------------------------')\n",
    "            found = '!'\n",
    "            # print('row', row)\n",
    "            try:\n",
    "                # print('Try, row:',row)\n",
    "                n_gram_row = one_row_n_grams(row, n, trim,vector_cell)\n",
    "                # print(n_gram_row)\n",
    "                # print('n_gram',n_gram_row)\n",
    "                for each in n_gram_row:\n",
    "                    # used when top lists are from file\n",
    "                    # print('cell:',each[0], len(each[0]),'level:',level[0], len(level[0]))\n",
    "                    if (each[0] == level[0]):\n",
    "                        # print('true')\n",
    "                        found = level[0]\n",
    "                    # used with top list is being discovered\n",
    "                    # print('cell:',each[0],'level:',level)\n",
    "                    # if (each[0] == level):\n",
    "                    #     print('true')\n",
    "                    #     found = level\n",
    "            except:\n",
    "                found = '!'\n",
    "            result.append(found)\n",
    "        # print('result', result)\n",
    "        return (result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def add_new_col_df(col_prefix , list,column_name, df, n, trim_text, word_vector):\n",
    "        i = 1\n",
    "        l = len(list)\n",
    "        for level in list:\n",
    "            # print('i', i, 'len', l)\n",
    "            # print(level)\n",
    "            # print(level[0])\n",
    "            if(i % 250 == 0 ):\n",
    "                print('Added ',i, 'out of ', l, 'columns. - ',dt.datetime.now())\n",
    "            new_col = col_prefix + str(level[0])\n",
    "            df[new_col] = match_ngrams(df[column_name], level, n, trim_text, word_vector)\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-25 22:23:37.561408\n"
     ]
    }
   ],
   "source": [
    "print(dt.datetime.now())\n",
    "# path is where raw files are \n",
    "path = 'C:/Users/peted/Documents/Python/Raw_Files/'\n",
    "# path_2 is where to put files after processing and where\n",
    "# variable files are kept: top_desc_list_v6.csv, top_hashtags_v5.csv, top_word_list_v6.csv\n",
    "path_2 = 'C:/Users/peted/Documents/Python/Raw_Files/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/peted/Documents/Python/Raw_Files/ guncontrol_2018-02-24_tweets_step_1.csv\n",
      "hashtag -----------------------------------------------------\n",
      "top: 399\n",
      "------------------------------------------------------\n",
      "Added  250 out of  399 columns. -  2018-03-25 22:26:47.057671\n",
      "done\n",
      "words -----------------------------------------------------\n",
      "top: 1711\n",
      "------------------------------------------------------\n",
      "Added  250 out of  1711 columns. -  2018-03-25 22:26:47.547440\n",
      "Added  500 out of  1711 columns. -  2018-03-25 22:26:48.016962\n",
      "Added  750 out of  1711 columns. -  2018-03-25 22:26:48.600368\n",
      "Added  1000 out of  1711 columns. -  2018-03-25 22:26:49.170325\n",
      "Added  1250 out of  1711 columns. -  2018-03-25 22:26:49.781461\n",
      "Added  1500 out of  1711 columns. -  2018-03-25 22:26:50.513493\n",
      "done\n",
      "user desc -----------------------------------------------------\n",
      "top: 1420\n",
      "------------------------------------------------------\n",
      "Added  250 out of  1420 columns. -  2018-03-25 22:26:51.653955\n",
      "Added  500 out of  1420 columns. -  2018-03-25 22:26:52.097157\n",
      "Added  750 out of  1420 columns. -  2018-03-25 22:26:52.572530\n",
      "Added  1000 out of  1420 columns. -  2018-03-25 22:26:53.181718\n",
      "Added  1250 out of  1420 columns. -  2018-03-25 22:26:53.811731\n",
      "Save file\n",
      "2018-03-25 22:26:54.308844\n",
      "done with file  guncontrol_2018-02-24_tweets_step_1.csv 1\n",
      "C:/Users/peted/Documents/Python/Raw_Files/ guncontrol_2018-02-24_tweets_step_2.csv\n",
      "hashtag -----------------------------------------------------\n",
      "top: 399\n",
      "------------------------------------------------------\n",
      "Added  250 out of  399 columns. -  2018-03-25 22:26:54.637894\n",
      "done\n",
      "words -----------------------------------------------------\n",
      "top: 1711\n",
      "------------------------------------------------------\n",
      "Added  250 out of  1711 columns. -  2018-03-25 22:26:55.985501\n",
      "Added  500 out of  1711 columns. -  2018-03-25 22:26:56.903421\n",
      "Added  750 out of  1711 columns. -  2018-03-25 22:26:57.976961\n",
      "Added  1000 out of  1711 columns. -  2018-03-25 22:26:58.863697\n",
      "Added  1250 out of  1711 columns. -  2018-03-25 22:26:59.767923\n",
      "Added  1500 out of  1711 columns. -  2018-03-25 22:27:00.750321\n",
      "done\n",
      "user desc -----------------------------------------------------\n",
      "top: 1420\n",
      "------------------------------------------------------\n",
      "Added  250 out of  1420 columns. -  2018-03-25 22:27:02.138974\n",
      "Added  500 out of  1420 columns. -  2018-03-25 22:27:02.639018\n",
      "Added  750 out of  1420 columns. -  2018-03-25 22:27:03.085304\n",
      "Added  1000 out of  1420 columns. -  2018-03-25 22:27:03.719409\n",
      "Added  1250 out of  1420 columns. -  2018-03-25 22:27:04.208359\n",
      "Save file\n",
      "2018-03-25 22:27:04.943961\n",
      "done with file  guncontrol_2018-02-24_tweets_step_2.csv 2\n",
      "C:/Users/peted/Documents/Python/Raw_Files/ guncontrol_2018-02-24_tweets_step_3.csv\n",
      "hashtag -----------------------------------------------------\n",
      "top: 399\n",
      "------------------------------------------------------\n",
      "Added  250 out of  399 columns. -  2018-03-25 22:27:05.252780\n",
      "done\n",
      "words -----------------------------------------------------\n",
      "top: 1711\n",
      "------------------------------------------------------\n",
      "Added  250 out of  1711 columns. -  2018-03-25 22:27:06.398082\n",
      "Added  500 out of  1711 columns. -  2018-03-25 22:27:07.560618\n",
      "Added  750 out of  1711 columns. -  2018-03-25 22:27:08.465497\n",
      "Added  1000 out of  1711 columns. -  2018-03-25 22:27:09.442928\n",
      "Added  1250 out of  1711 columns. -  2018-03-25 22:27:10.617476\n",
      "Added  1500 out of  1711 columns. -  2018-03-25 22:27:11.638334\n",
      "done\n",
      "user desc -----------------------------------------------------\n",
      "top: 1420\n",
      "------------------------------------------------------\n",
      "Added  250 out of  1420 columns. -  2018-03-25 22:27:13.245794\n",
      "Added  500 out of  1420 columns. -  2018-03-25 22:27:13.802410\n",
      "Added  750 out of  1420 columns. -  2018-03-25 22:27:14.482533\n",
      "Added  1000 out of  1420 columns. -  2018-03-25 22:27:15.084966\n",
      "Added  1250 out of  1420 columns. -  2018-03-25 22:27:15.653358\n",
      "Save file\n",
      "2018-03-25 22:27:16.068516\n",
      "done with file  guncontrol_2018-02-24_tweets_step_3.csv 3\n",
      "C:/Users/peted/Documents/Python/Raw_Files/ guncontrol_2018-02-24_tweets_step_4.csv\n",
      "hashtag -----------------------------------------------------\n",
      "top: 399\n",
      "------------------------------------------------------\n",
      "Added  250 out of  399 columns. -  2018-03-25 22:27:16.349703\n",
      "done\n",
      "words -----------------------------------------------------\n",
      "top: 1711\n",
      "------------------------------------------------------\n",
      "Added  250 out of  1711 columns. -  2018-03-25 22:27:17.191978\n",
      "Added  500 out of  1711 columns. -  2018-03-25 22:27:18.034974\n",
      "Added  750 out of  1711 columns. -  2018-03-25 22:27:18.790138\n",
      "Added  1000 out of  1711 columns. -  2018-03-25 22:27:19.540898\n",
      "Added  1250 out of  1711 columns. -  2018-03-25 22:27:20.361449\n",
      "Added  1500 out of  1711 columns. -  2018-03-25 22:27:21.109461\n",
      "done\n",
      "user desc -----------------------------------------------------\n",
      "top: 1420\n",
      "------------------------------------------------------\n",
      "Added  250 out of  1420 columns. -  2018-03-25 22:27:22.232740\n",
      "Added  500 out of  1420 columns. -  2018-03-25 22:27:22.770440\n",
      "Added  750 out of  1420 columns. -  2018-03-25 22:27:23.501582\n",
      "Added  1000 out of  1420 columns. -  2018-03-25 22:27:24.069506\n",
      "Added  1250 out of  1420 columns. -  2018-03-25 22:27:24.650807\n",
      "Save file\n",
      "2018-03-25 22:27:25.125390\n",
      "done with file  guncontrol_2018-02-24_tweets_step_4.csv 4\n",
      "done with all cycles\n"
     ]
    }
   ],
   "source": [
    "objects = fnmatch.filter(os.listdir(path), '*_step_*')\n",
    "i = 1\n",
    "for load_file in objects:\n",
    "    # to load goldset\n",
    "    # load_file = 'Training_Gun_Tweets_v6.csv'\n",
    "    print(path,load_file)\n",
    "\n",
    "    df_tweet = pd.read_csv(path + load_file, encoding='utf-8')\n",
    "\n",
    "    # for testing\n",
    "    df_tweet = df_tweet[:10]\n",
    "\n",
    "    col_count = 0\n",
    "\n",
    "    print('hashtag -----------------------------------------------------')\n",
    "    n = 1\n",
    "    trim_text = True\n",
    "    word_vector = False\n",
    "    # top list\n",
    "    file = 'top_hashtags_v5.csv'\n",
    "    with open(path_2 + file, 'r', encoding='utf-8') as f:\n",
    "      reader = csv.reader(f)\n",
    "      top_hashtags = list(reader)\n",
    "    print('top:',len(top_hashtags))\n",
    "    # print('top:','len:', len(top_hashtags),top_hashtags)\n",
    "    print('------------------------------------------------------')\n",
    "    # create new columns (Random Forest can only take 32 levels for any given column\n",
    "    add_new_col_df('#_',top_hashtags, 'hashtags', df_tweet, n, trim_text, word_vector)\n",
    "    print('done')\n",
    "\n",
    "    # get list of words\n",
    "    print('words -----------------------------------------------------')\n",
    "    # ngrams\n",
    "    n = 1\n",
    "    min_count = 0.5\n",
    "    trim_text = True\n",
    "    word_vector = False\n",
    "    # # top list\n",
    "    file = 'top_word_list_v6.csv'\n",
    "    with open(path_2 + file, 'r', encoding='utf-8') as f:\n",
    "      reader = csv.reader(f)\n",
    "      top_list = list(reader)\n",
    "    print('top:', len(top_list))\n",
    "    # print('top:','len:', len(top_list),top_list)\n",
    "    print('------------------------------------------------------')\n",
    "    # create new columns (Random Forest can only take 32 levels for any given column\n",
    "    add_new_col_df('t_', top_list, 'text', df_tweet, n, trim_text, word_vector)\n",
    "    print('done')\n",
    "\n",
    "    first_col = df_tweet.shape[1]+1\n",
    "    # # # get list of words in user desc\n",
    "    print('user desc -----------------------------------------------------')\n",
    "    # ngrams\n",
    "    n = 1\n",
    "    min_count = 0.5\n",
    "    trim_text = False\n",
    "    word_vector = False\n",
    "    # top list\n",
    "    file = 'top_desc_list_v6.csv'\n",
    "    with open(path_2 + file, 'r', encoding='utf-8') as f:\n",
    "      reader = csv.reader(f)\n",
    "      top_desc = list(reader)\n",
    "    print('top:', len(top_desc))\n",
    "    # print('top:','len:', len(top_desc),top_desc)\n",
    "    print('------------------------------------------------------')\n",
    "    # create new columns (Random Forest can only take 32 levels for any given column\n",
    "    add_new_col_df('u_',top_desc, 'user_desc', df_tweet, n, trim_text, word_vector)\n",
    "\n",
    "    print('Save file')\n",
    "    load = load_file[:-4]+'_ready_for_RF_2.csv'\n",
    "    df_tweet.to_csv(path_2 + load, sep=',', na_rep='', index=False, encoding='utf-8')\n",
    "\n",
    "    print(dt.datetime.now())\n",
    "    print('done with file ',load_file, i)\n",
    "    i += 1\n",
    "    # break\n",
    "print('done with all cycles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
